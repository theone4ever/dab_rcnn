{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN - Train on Shapes Dataset\n",
    "\n",
    "\n",
    "This notebook shows how to train Mask R-CNN on your own dataset. To keep things simple we use a synthetic dataset of shapes (squares, triangles, and circles) which enables fast training. You'd still need a GPU, though, because the network backbone is a Resnet101, which would be too slow to train on a CPU. On a GPU, you can start to get okay-ish results in a few minutes, and good results in less than an hour.\n",
    "\n",
    "The code of the *Shapes* dataset is included below. It generates images on the fly, so it doesn't require downloading any data. And it can generate images of any size, so we pick a small image size to train faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from config import Config\n",
    "import utils\n",
    "import model as modellib\n",
    "import visualize\n",
    "from model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.getcwd()\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE_SHAPES                [[64 64]\n",
      " [32 32]\n",
      " [16 16]\n",
      " [ 8  8]\n",
      " [ 4  4]]\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     8\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "GPU_COUNT                      1\n",
      "IMAGES_PER_GPU                 8\n",
      "IMAGE_MAX_DIM                  256\n",
      "IMAGE_MIN_DIM                  256\n",
      "IMAGE_PADDING                  True\n",
      "IMAGE_SHAPE                    [256 256   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           shapes\n",
      "NUM_CLASSES                    4\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                100\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               5\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ShapesConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"shapes\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 8\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 3  # background + 3 shapes\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 256\n",
    "    IMAGE_MAX_DIM = 256\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 5\n",
    "    \n",
    "config = ShapesConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Create a synthetic dataset\n",
    "\n",
    "Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n",
    "\n",
    "* load_image()\n",
    "* load_mask()\n",
    "* image_reference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ShapesDataset(utils.Dataset):\n",
    "    \"\"\"Generates the shapes synthetic dataset. The dataset consists of simple\n",
    "    shapes (triangles, squares, circles) placed randomly on a blank surface.\n",
    "    The images are generated on the fly. No file access required.\n",
    "    \"\"\"\n",
    "\n",
    "    def load_shapes(self, count, height, width):\n",
    "        \"\"\"Generate the requested number of synthetic images.\n",
    "        count: number of images to generate.\n",
    "        height, width: the size of the generated images.\n",
    "        \"\"\"\n",
    "        # Add classes\n",
    "        self.add_class(\"shapes\", 1, \"square\")\n",
    "        self.add_class(\"shapes\", 2, \"circle\")\n",
    "        self.add_class(\"shapes\", 3, \"triangle\")\n",
    "\n",
    "        # Add images\n",
    "        # Generate random specifications of images (i.e. color and\n",
    "        # list of shapes sizes and locations). This is more compact than\n",
    "        # actual images. Images are generated on the fly in load_image().\n",
    "        for i in range(count):\n",
    "            bg_color, shapes = self.random_image(height, width)\n",
    "            self.add_image(\"shapes\", image_id=i, path=None,\n",
    "                           width=width, height=height,\n",
    "                           bg_color=bg_color, shapes=shapes)\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        \"\"\"Generate an image from the specs of the given image ID.\n",
    "        Typically this function loads the image from a file, but\n",
    "        in this case it generates the image on the fly from the\n",
    "        specs in image_info.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        bg_color = np.array(info['bg_color']).reshape([1, 1, 3])\n",
    "        image = np.ones([info['height'], info['width'], 3], dtype=np.uint8)\n",
    "        image = image * bg_color.astype(np.uint8)\n",
    "        for shape, color, dims in info['shapes']:\n",
    "            image = self.draw_shape(image, shape, dims, color)\n",
    "        return image\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the shapes data of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"shapes\":\n",
    "            return info[\"shapes\"]\n",
    "        else:\n",
    "            super(self.__class__).image_reference(self, image_id)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for shapes of the given image ID.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        shapes = info['shapes']\n",
    "        count = len(shapes)\n",
    "        mask = np.zeros([info['height'], info['width'], count], dtype=np.uint8)\n",
    "        for i, (shape, _, dims) in enumerate(info['shapes']):\n",
    "            mask[:, :, i:i+1] = self.draw_shape(mask[:, :, i:i+1].copy(),\n",
    "                                                shape, dims, 1)\n",
    "        # Handle occlusions\n",
    "        occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)\n",
    "        for i in range(count-2, -1, -1):\n",
    "            mask[:, :, i] = mask[:, :, i] * occlusion\n",
    "            occlusion = np.logical_and(occlusion, np.logical_not(mask[:, :, i]))\n",
    "        # Map class names to class IDs.\n",
    "        class_ids = np.array([self.class_names.index(s[0]) for s in shapes])\n",
    "        return mask, class_ids.astype(np.int32)\n",
    "\n",
    "    def draw_shape(self, image, shape, dims, color):\n",
    "        \"\"\"Draws a shape from the given specs.\"\"\"\n",
    "        # Get the center x, y and the size s\n",
    "        x, y, s = dims\n",
    "        if shape == 'square':\n",
    "            cv2.rectangle(image, (x-s, y-s), (x+s, y+s), color, -1)\n",
    "        elif shape == \"circle\":\n",
    "            cv2.circle(image, (x, y), s, color, -1)\n",
    "        elif shape == \"triangle\":\n",
    "            points = np.array([[(x, y-s),\n",
    "                                (x-s/math.sin(math.radians(60)), y+s),\n",
    "                                (x+s/math.sin(math.radians(60)), y+s),\n",
    "                                ]], dtype=np.int32)\n",
    "            cv2.fillPoly(image, points, color)\n",
    "        return image\n",
    "\n",
    "    def random_shape(self, height, width):\n",
    "        \"\"\"Generates specifications of a random shape that lies within\n",
    "        the given height and width boundaries.\n",
    "        Returns a tuple of three valus:\n",
    "        * The shape name (square, circle, ...)\n",
    "        * Shape color: a tuple of 3 values, RGB.\n",
    "        * Shape dimensions: A tuple of values that define the shape size\n",
    "                            and location. Differs per shape type.\n",
    "        \"\"\"\n",
    "        # Shape\n",
    "        shape = random.choice([\"square\", \"circle\", \"triangle\"])\n",
    "        # Color\n",
    "        color = tuple([random.randint(0, 255) for _ in range(3)])\n",
    "        # Center x, y\n",
    "        buffer = 20\n",
    "        y = random.randint(buffer, height - buffer - 1)\n",
    "        x = random.randint(buffer, width - buffer - 1)\n",
    "        # Size\n",
    "        s = random.randint(buffer, height//4)\n",
    "        return shape, color, (x, y, s)\n",
    "\n",
    "    def random_image(self, height, width):\n",
    "        \"\"\"Creates random specifications of an image with multiple shapes.\n",
    "        Returns the background color of the image and a list of shape\n",
    "        specifications that can be used to draw the image.\n",
    "        \"\"\"\n",
    "        # Pick random background color\n",
    "        bg_color = np.array([random.randint(0, 255) for _ in range(3)])\n",
    "        # Generate a few random shapes and record their\n",
    "        # bounding boxes\n",
    "        shapes = []\n",
    "        boxes = []\n",
    "        N = random.randint(1, 4)\n",
    "        for _ in range(N):\n",
    "            shape, color, dims = self.random_shape(height, width)\n",
    "            shapes.append((shape, color, dims))\n",
    "            x, y, s = dims\n",
    "            boxes.append([y-s, x-s, y+s, x+s])\n",
    "        # Apply non-max suppression wit 0.3 threshold to avoid\n",
    "        # shapes covering each other\n",
    "        keep_ixs = utils.non_max_suppression(np.array(boxes), np.arange(N), 0.3)\n",
    "        shapes = [s for i, s in enumerate(shapes) if i in keep_ixs]\n",
    "        return bg_color, shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "dataset_train = ShapesDataset()\n",
    "dataset_train.load_shapes(500, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = ShapesDataset()\n",
    "dataset_val.load_shapes(50, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAy0AAACnCAYAAAAc07MlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f30e0c66390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAy0AAACnCAYAAAAc07MlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f30e133b080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAy0AAACnCAYAAAAc07MlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAOiaaAEAALr2fyogKpoN9KVUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f30e00f8518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAy0AAACnCAYAAAAc07MlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFgAAoGuiBQAA6Nr/ARUfi+088IzuAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f30e24abef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and display random samples\n",
    "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
    "for image_id in image_ids:\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ceate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                          model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Which weights to start with?\n",
    "init_with = \"coco\"  # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last()[1], by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train in two stages:\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 0. LR=0.001\n",
      "\n",
      "Checkpoint Path: /home/keven/data_science_bowl_2018/Mask_RCNN/logs/shapes20180216T1340/mask_rcnn_shapes_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keven/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:95: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/keven/miniconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training.py:2033: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error processing image {'id': 122, 'source': 'shapes', 'path': None, 'width': 256, 'height': 256, 'bg_color': array([210, 155, 180]), 'shapes': [('circle', (208, 67, 39), (172, 160, 34)), ('circle', (174, 133, 1), (182, 95, 24)), ('square', (190, 4, 18), (207, 106, 49))]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/keven/data_science_bowl_2018/Mask_RCNN/model.py\", line 1632, in data_generator\n",
      "    use_mini_mask=config.USE_MINI_MASK)\n",
      "  File \"/home/keven/data_science_bowl_2018/Mask_RCNN/model.py\", line 1220, in load_image_gt\n",
      "    mask = utils.minimize_mask(bbox, mask, config.MINI_MASK_SHAPE)\n",
      "  File \"/home/keven/data_science_bowl_2018/Mask_RCNN/utils.py\", line 462, in minimize_mask\n",
      "    raise Exception(\"Invalid bounding box with area of zero\")\n",
      "Exception: Invalid bounding box with area of zero\n",
      "ERROR:root:Error processing image {'id': 216, 'source': 'shapes', 'path': None, 'width': 256, 'height': 256, 'bg_color': array([227,  37, 103]), 'shapes': [('triangle', (233, 138, 124), (130, 220, 31)), ('triangle', (107, 158, 173), (209, 153, 30)), ('circle', (139, 220, 153), (179, 123, 61)), ('square', (181, 207, 117), (230, 187, 58))]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/keven/data_science_bowl_2018/Mask_RCNN/model.py\", line 1632, in data_generator\n",
      "    use_mini_mask=config.USE_MINI_MASK)\n",
      "  File \"/home/keven/data_science_bowl_2018/Mask_RCNN/model.py\", line 1220, in load_image_gt\n",
      "    mask = utils.minimize_mask(bbox, mask, config.MINI_MASK_SHAPE)\n",
      "  File \"/home/keven/data_science_bowl_2018/Mask_RCNN/utils.py\", line 462, in minimize_mask\n",
      "    raise Exception(\"Invalid bounding box with area of zero\")\n",
      "Exception: Invalid bounding box with area of zero\n",
      "ERROR:root:Error processing image {'id': 365, 'source': 'shapes', 'path': None, 'width': 256, 'height': 256, 'bg_color': array([242, 157, 180]), 'shapes': [('triangle', (218, 249, 197), (56, 159, 23)), ('triangle', (185, 68, 26), (150, 228, 64)), ('triangle', (162, 143, 237), (35, 151, 58)), ('triangle', (223, 246, 229), (133, 72, 45))]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/keven/data_science_bowl_2018/Mask_RCNN/model.py\", line 1632, in data_generator\n",
      "    use_mini_mask=config.USE_MINI_MASK)\n",
      "  File \"/home/keven/data_science_bowl_2018/Mask_RCNN/model.py\", line 1220, in load_image_gt\n",
      "    mask = utils.minimize_mask(bbox, mask, config.MINI_MASK_SHAPE)\n",
      "  File \"/home/keven/data_science_bowl_2018/Mask_RCNN/utils.py\", line 462, in minimize_mask\n",
      "    raise Exception(\"Invalid bounding box with area of zero\")\n",
      "Exception: Invalid bounding box with area of zeroERROR:root:Error processing image {'id': 78, 'source': 'shapes', 'path': None, 'width': 256, 'height': 256, 'bg_color': array([ 75, 125, 249]), 'shapes': [('circle', (25, 199, 204), (87, 180, 24)), ('square', (216, 14, 7), (151, 222, 62)), ('square', (67, 200, 143), (79, 201, 50))]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/keven/data_science_bowl_2018/Mask_RCNN/model.py\", line 1632, in data_generator\n",
      "    use_mini_mask=config.USE_MINI_MASK)\n",
      "  File \"/home/keven/data_science_bowl_2018/Mask_RCNN/model.py\", line 1220, in load_image_gt\n",
      "    mask = utils.minimize_mask(bbox, mask, config.MINI_MASK_SHAPE)\n",
      "  File \"/home/keven/data_science_bowl_2018/Mask_RCNN/utils.py\", line 462, in minimize_mask\n",
      "    raise Exception(\"Invalid bounding box with area of zero\")\n",
      "Exception: Invalid bounding box with area of zero\n",
      "\n",
      "ERROR:root:Error processing image {'id': 216, 'source': 'shapes', 'path': None, 'width': 256, 'height': 256, 'bg_color': array([227,  37, 103]), 'shapes': [('triangle', (233, 138, 124), (130, 220, 31)), ('triangle', (107, 158, 173), (209, 153, 30)), ('circle', (139, 220, 153), (179, 123, 61)), ('square', (181, 207, 117), (230, 187, 58))]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/keven/data_science_bowl_2018/Mask_RCNN/model.py\", line 1632, in data_generator\n",
      "    use_mini_mask=config.USE_MINI_MASK)\n",
      "  File \"/home/keven/data_science_bowl_2018/Mask_RCNN/model.py\", line 1220, in load_image_gt\n",
      "    mask = utils.minimize_mask(bbox, mask, config.MINI_MASK_SHAPE)\n",
      "  File \"/home/keven/data_science_bowl_2018/Mask_RCNN/utils.py\", line 462, in minimize_mask\n",
      "    raise Exception(\"Invalid bounding box with area of zero\")\n",
      "Exception: Invalid bounding box with area of zeroERROR:root:Error processing image {'id': 39, 'source': 'shapes', 'path': None, 'width': 256, 'height': 256, 'bg_color': array([211, 219, 175]), 'shapes': [('circle', (239, 24, 25), (182, 145, 22)), ('circle', (51, 201, 2), (233, 193, 48)), ('square', (101, 36, 112), (187, 164, 47)), ('square', (135, 150, 146), (118, 168, 32))]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/keven/data_science_bowl_2018/Mask_RCNN/model.py\", line 1632, in data_generator\n",
      "    use_mini_mask=config.USE_MINI_MASK)\n",
      "  File \"/home/keven/data_science_bowl_2018/Mask_RCNN/model.py\", line 1220, in load_image_gt\n",
      "    mask = utils.minimize_mask(bbox, mask, config.MINI_MASK_SHAPE)\n",
      "  File \"/home/keven/data_science_bowl_2018/Mask_RCNN/utils.py\", line 462, in minimize_mask\n",
      "    raise Exception(\"Invalid bounding box with area of zero\")\n",
      "Exception: Invalid bounding box with area of zero\n",
      "\n",
      "ERROR:root:Error processing image {'id': 365, 'source': 'shapes', 'path': None, 'width': 256, 'height': 256, 'bg_color': array([242, 157, 180]), 'shapes': [('triangle', (218, 249, 197), (56, 159, 23)), ('triangle', (185, 68, 26), (150, 228, 64)), ('triangle', (162, 143, 237), (35, 151, 58)), ('triangle', (223, 246, 229), (133, 72, 45))]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/keven/data_science_bowl_2018/Mask_RCNN/model.py\", line 1632, in data_generator\n",
      "    use_mini_mask=config.USE_MINI_MASK)\n",
      "  File \"/home/keven/data_science_bowl_2018/Mask_RCNN/model.py\", line 1220, in load_image_gt\n",
      "    mask = utils.minimize_mask(bbox, mask, config.MINI_MASK_SHAPE)\n",
      "  File \"/home/keven/data_science_bowl_2018/Mask_RCNN/utils.py\", line 462, in minimize_mask\n",
      "    raise Exception(\"Invalid bounding box with area of zero\")\n",
      "Exception: Invalid bounding box with area of zero\n",
      "ERROR:root:Error processing image {'id': 417, 'source': 'shapes', 'path': None, 'width': 256, 'height': 256, 'bg_color': array([195,  72, 194]), 'shapes': [('triangle', (3, 142, 5), (29, 215, 46)), ('triangle', (12, 45, 168), (191, 126, 29)), ('triangle', (38, 10, 94), (23, 106, 20)), ('square', (199, 46, 12), (24, 75, 53))]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/keven/data_science_bowl_2018/Mask_RCNN/model.py\", line 1632, in data_generator\n",
      "    use_mini_mask=config.USE_MINI_MASK)\n",
      "  File \"/home/keven/data_science_bowl_2018/Mask_RCNN/model.py\", line 1220, in load_image_gt\n",
      "    mask = utils.minimize_mask(bbox, mask, config.MINI_MASK_SHAPE)\n",
      "  File \"/home/keven/data_science_bowl_2018/Mask_RCNN/utils.py\", line 462, in minimize_mask\n",
      "    raise Exception(\"Invalid bounding box with area of zero\")\n",
      "Exception: Invalid bounding box with area of zero\n",
      "ERROR:root:Error processing image {'id': 78, 'source': 'shapes', 'path': None, 'width': 256, 'height': 256, 'bg_color': array([ 75, 125, 249]), 'shapes': [('circle', (25, 199, 204), (87, 180, 24)), ('square', (216, 14, 7), (151, 222, 62)), ('square', (67, 200, 143), (79, 201, 50))]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/keven/data_science_bowl_2018/Mask_RCNN/model.py\", line 1632, in data_generator\n",
      "    use_mini_mask=config.USE_MINI_MASK)\n",
      "  File \"/home/keven/data_science_bowl_2018/Mask_RCNN/model.py\", line 1220, in load_image_gt\n",
      "    mask = utils.minimize_mask(bbox, mask, config.MINI_MASK_SHAPE)\n",
      "  File \"/home/keven/data_science_bowl_2018/Mask_RCNN/utils.py\", line 462, in minimize_mask\n",
      "    raise Exception(\"Invalid bounding box with area of zero\")\n",
      "Exception: Invalid bounding box with area of zero\n",
      "ERROR:root:Error processing image {'id': 39, 'source': 'shapes', 'path': None, 'width': 256, 'height': 256, 'bg_color': array([211, 219, 175]), 'shapes': [('circle', (239, 24, 25), (182, 145, 22)), ('circle', (51, 201, 2), (233, 193, 48)), ('square', (101, 36, 112), (187, 164, 47)), ('square', (135, 150, 146), (118, 168, 32))]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/keven/data_science_bowl_2018/Mask_RCNN/model.py\", line 1632, in data_generator\n",
      "    use_mini_mask=config.USE_MINI_MASK)\n",
      "  File \"/home/keven/data_science_bowl_2018/Mask_RCNN/model.py\", line 1220, in load_image_gt\n",
      "    mask = utils.minimize_mask(bbox, mask, config.MINI_MASK_SHAPE)\n",
      "  File \"/home/keven/data_science_bowl_2018/Mask_RCNN/utils.py\", line 462, in minimize_mask\n",
      "    raise Exception(\"Invalid bounding box with area of zero\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception: Invalid bounding box with area of zero\n",
      "ERROR:root:Error processing image {'id': 216, 'source': 'shapes', 'path': None, 'width': 256, 'height': 256, 'bg_color': array([227,  37, 103]), 'shapes': [('triangle', (233, 138, 124), (130, 220, 31)), ('triangle', (107, 158, 173), (209, 153, 30)), ('circle', (139, 220, 153), (179, 123, 61)), ('square', (181, 207, 117), (230, 187, 58))]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/keven/data_science_bowl_2018/Mask_RCNN/model.py\", line 1632, in data_generator\n",
      "    use_mini_mask=config.USE_MINI_MASK)\n",
      "  File \"/home/keven/data_science_bowl_2018/Mask_RCNN/model.py\", line 1220, in load_image_gt\n",
      "    mask = utils.minimize_mask(bbox, mask, config.MINI_MASK_SHAPE)\n",
      "  File \"/home/keven/data_science_bowl_2018/Mask_RCNN/utils.py\", line 462, in minimize_mask\n",
      "    raise Exception(\"Invalid bounding box with area of zero\")\n",
      "Exception: Invalid bounding box with area of zero\n",
      "ERROR:root:Error processing image {'id': 417, 'source': 'shapes', 'path': None, 'width': 256, 'height': 256, 'bg_color': array([195,  72, 194]), 'shapes': [('triangle', (3, 142, 5), (29, 215, 46)), ('triangle', (12, 45, 168), (191, 126, 29)), ('triangle', (38, 10, 94), (23, 106, 20)), ('square', (199, 46, 12), (24, 75, 53))]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/keven/data_science_bowl_2018/Mask_RCNN/model.py\", line 1632, in data_generator\n",
      "    use_mini_mask=config.USE_MINI_MASK)\n",
      "  File \"/home/keven/data_science_bowl_2018/Mask_RCNN/model.py\", line 1220, in load_image_gt\n",
      "    mask = utils.minimize_mask(bbox, mask, config.MINI_MASK_SHAPE)\n",
      "  File \"/home/keven/data_science_bowl_2018/Mask_RCNN/utils.py\", line 462, in minimize_mask\n",
      "    raise Exception(\"Invalid bounding box with area of zero\")\n",
      "Exception: Invalid bounding box with area of zero\n",
      "ERROR:root:Error processing image {'id': 417, 'source': 'shapes', 'path': None, 'width': 256, 'height': 256, 'bg_color': array([195,  72, 194]), 'shapes': [('triangle', (3, 142, 5), (29, 215, 46)), ('triangle', (12, 45, 168), (191, 126, 29)), ('triangle', (38, 10, 94), (23, 106, 20)), ('square', (199, 46, 12), (24, 75, 53))]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/keven/data_science_bowl_2018/Mask_RCNN/model.py\", line 1632, in data_generator\n",
      "    use_mini_mask=config.USE_MINI_MASK)\n",
      "  File \"/home/keven/data_science_bowl_2018/Mask_RCNN/model.py\", line 1220, in load_image_gt\n",
      "    mask = utils.minimize_mask(bbox, mask, config.MINI_MASK_SHAPE)\n",
      "  File \"/home/keven/data_science_bowl_2018/Mask_RCNN/utils.py\", line 462, in minimize_mask\n",
      "    raise Exception(\"Invalid bounding box with area of zero\")\n",
      "Exception: Invalid bounding box with area of zero\n",
      "ERROR:root:Error processing image {'id': 172, 'source': 'shapes', 'path': None, 'width': 256, 'height': 256, 'bg_color': array([204, 177, 103]), 'shapes': [('square', (244, 218, 230), (104, 152, 27)), ('circle', (160, 57, 232), (201, 118, 34)), ('triangle', (194, 238, 14), (213, 204, 51)), ('triangle', (250, 86, 169), (195, 93, 63))]}\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/keven/data_science_bowl_2018/Mask_RCNN/model.py\", line 1632, in data_generator\n",
      "    use_mini_mask=config.USE_MINI_MASK)\n",
      "  File \"/home/keven/data_science_bowl_2018/Mask_RCNN/model.py\", line 1220, in load_image_gt\n",
      "    mask = utils.minimize_mask(bbox, mask, config.MINI_MASK_SHAPE)\n",
      "  File \"/home/keven/data_science_bowl_2018/Mask_RCNN/utils.py\", line 462, in minimize_mask\n",
      "    raise Exception(\"Invalid bounding box with area of zero\")\n",
      "Exception: Invalid bounding box with area of zero\n",
      "Process Process-33:\n",
      "Process Process-34:\n",
      "Process Process-32:\n",
      "Process Process-35:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/keven/miniconda3/envs/tensorflow/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/keven/miniconda3/envs/tensorflow/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/keven/miniconda3/envs/tensorflow/lib/python3.6/site-packages/keras/utils/data_utils.py\", line 657, in _data_generator_task\n",
      "    time.sleep(self.wait_time)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/keven/miniconda3/envs/tensorflow/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/keven/miniconda3/envs/tensorflow/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/keven/miniconda3/envs/tensorflow/lib/python3.6/site-packages/keras/utils/data_utils.py\", line 655, in _data_generator_task\n",
      "    self.queue.put((True, generator_output))\n",
      "  File \"<string>\", line 2, in put\n",
      "  File \"/home/keven/miniconda3/envs/tensorflow/lib/python3.6/multiprocessing/managers.py\", line 757, in _callmethod\n",
      "    kind, result = conn.recv()\n",
      "  File \"/home/keven/miniconda3/envs/tensorflow/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/keven/miniconda3/envs/tensorflow/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/keven/miniconda3/envs/tensorflow/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/keven/miniconda3/envs/tensorflow/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/keven/miniconda3/envs/tensorflow/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/keven/miniconda3/envs/tensorflow/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/keven/miniconda3/envs/tensorflow/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/keven/miniconda3/envs/tensorflow/lib/python3.6/site-packages/keras/utils/data_utils.py\", line 655, in _data_generator_task\n",
      "    self.queue.put((True, generator_output))\n",
      "  File \"<string>\", line 2, in put\n",
      "  File \"/home/keven/miniconda3/envs/tensorflow/lib/python3.6/multiprocessing/managers.py\", line 757, in _callmethod\n",
      "    kind, result = conn.recv()\n",
      "  File \"/home/keven/miniconda3/envs/tensorflow/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/keven/miniconda3/envs/tensorflow/lib/python3.6/site-packages/keras/utils/data_utils.py\", line 655, in _data_generator_task\n",
      "    self.queue.put((True, generator_output))\n",
      "  File \"<string>\", line 2, in put\n",
      "  File \"/home/keven/miniconda3/envs/tensorflow/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/keven/miniconda3/envs/tensorflow/lib/python3.6/multiprocessing/managers.py\", line 757, in _callmethod\n",
      "    kind, result = conn.recv()\n",
      "  File \"/home/keven/miniconda3/envs/tensorflow/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/keven/miniconda3/envs/tensorflow/lib/python3.6/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/keven/miniconda3/envs/tensorflow/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/keven/miniconda3/envs/tensorflow/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-2101606c7d8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             layers='heads')\n\u001b[0m",
      "\u001b[0;32m~/data_science_bowl_2018/Mask_RCNN/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_dataset, val_dataset, learning_rate, epochs, layers)\u001b[0m\n\u001b[1;32m   2236\u001b[0m             \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2237\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2238\u001b[0;31m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2239\u001b[0m         )\n\u001b[1;32m   2240\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2175\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2176\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2177\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1847\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1848\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1849\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2475\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2476\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=1, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 2. LR=0.0001\n",
      "\n",
      "Checkpoint Path: /home/keven/data_science_bowl_2018/Mask_RCNN/logs/shapes20180216T1319/mask_rcnn_shapes_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "conv1                  (Conv2D)\n",
      "bn_conv1               (BatchNorm)\n",
      "res2a_branch2a         (Conv2D)\n",
      "bn2a_branch2a          (BatchNorm)\n",
      "res2a_branch2b         (Conv2D)\n",
      "bn2a_branch2b          (BatchNorm)\n",
      "res2a_branch2c         (Conv2D)\n",
      "res2a_branch1          (Conv2D)\n",
      "bn2a_branch2c          (BatchNorm)\n",
      "bn2a_branch1           (BatchNorm)\n",
      "res2b_branch2a         (Conv2D)\n",
      "bn2b_branch2a          (BatchNorm)\n",
      "res2b_branch2b         (Conv2D)\n",
      "bn2b_branch2b          (BatchNorm)\n",
      "res2b_branch2c         (Conv2D)\n",
      "bn2b_branch2c          (BatchNorm)\n",
      "res2c_branch2a         (Conv2D)\n",
      "bn2c_branch2a          (BatchNorm)\n",
      "res2c_branch2b         (Conv2D)\n",
      "bn2c_branch2b          (BatchNorm)\n",
      "res2c_branch2c         (Conv2D)\n",
      "bn2c_branch2c          (BatchNorm)\n",
      "res3a_branch2a         (Conv2D)\n",
      "bn3a_branch2a          (BatchNorm)\n",
      "res3a_branch2b         (Conv2D)\n",
      "bn3a_branch2b          (BatchNorm)\n",
      "res3a_branch2c         (Conv2D)\n",
      "res3a_branch1          (Conv2D)\n",
      "bn3a_branch2c          (BatchNorm)\n",
      "bn3a_branch1           (BatchNorm)\n",
      "res3b_branch2a         (Conv2D)\n",
      "bn3b_branch2a          (BatchNorm)\n",
      "res3b_branch2b         (Conv2D)\n",
      "bn3b_branch2b          (BatchNorm)\n",
      "res3b_branch2c         (Conv2D)\n",
      "bn3b_branch2c          (BatchNorm)\n",
      "res3c_branch2a         (Conv2D)\n",
      "bn3c_branch2a          (BatchNorm)\n",
      "res3c_branch2b         (Conv2D)\n",
      "bn3c_branch2b          (BatchNorm)\n",
      "res3c_branch2c         (Conv2D)\n",
      "bn3c_branch2c          (BatchNorm)\n",
      "res3d_branch2a         (Conv2D)\n",
      "bn3d_branch2a          (BatchNorm)\n",
      "res3d_branch2b         (Conv2D)\n",
      "bn3d_branch2b          (BatchNorm)\n",
      "res3d_branch2c         (Conv2D)\n",
      "bn3d_branch2c          (BatchNorm)\n",
      "res4a_branch2a         (Conv2D)\n",
      "bn4a_branch2a          (BatchNorm)\n",
      "res4a_branch2b         (Conv2D)\n",
      "bn4a_branch2b          (BatchNorm)\n",
      "res4a_branch2c         (Conv2D)\n",
      "res4a_branch1          (Conv2D)\n",
      "bn4a_branch2c          (BatchNorm)\n",
      "bn4a_branch1           (BatchNorm)\n",
      "res4b_branch2a         (Conv2D)\n",
      "bn4b_branch2a          (BatchNorm)\n",
      "res4b_branch2b         (Conv2D)\n",
      "bn4b_branch2b          (BatchNorm)\n",
      "res4b_branch2c         (Conv2D)\n",
      "bn4b_branch2c          (BatchNorm)\n",
      "res4c_branch2a         (Conv2D)\n",
      "bn4c_branch2a          (BatchNorm)\n",
      "res4c_branch2b         (Conv2D)\n",
      "bn4c_branch2b          (BatchNorm)\n",
      "res4c_branch2c         (Conv2D)\n",
      "bn4c_branch2c          (BatchNorm)\n",
      "res4d_branch2a         (Conv2D)\n",
      "bn4d_branch2a          (BatchNorm)\n",
      "res4d_branch2b         (Conv2D)\n",
      "bn4d_branch2b          (BatchNorm)\n",
      "res4d_branch2c         (Conv2D)\n",
      "bn4d_branch2c          (BatchNorm)\n",
      "res4e_branch2a         (Conv2D)\n",
      "bn4e_branch2a          (BatchNorm)\n",
      "res4e_branch2b         (Conv2D)\n",
      "bn4e_branch2b          (BatchNorm)\n",
      "res4e_branch2c         (Conv2D)\n",
      "bn4e_branch2c          (BatchNorm)\n",
      "res4f_branch2a         (Conv2D)\n",
      "bn4f_branch2a          (BatchNorm)\n",
      "res4f_branch2b         (Conv2D)\n",
      "bn4f_branch2b          (BatchNorm)\n",
      "res4f_branch2c         (Conv2D)\n",
      "bn4f_branch2c          (BatchNorm)\n",
      "res4g_branch2a         (Conv2D)\n",
      "bn4g_branch2a          (BatchNorm)\n",
      "res4g_branch2b         (Conv2D)\n",
      "bn4g_branch2b          (BatchNorm)\n",
      "res4g_branch2c         (Conv2D)\n",
      "bn4g_branch2c          (BatchNorm)\n",
      "res4h_branch2a         (Conv2D)\n",
      "bn4h_branch2a          (BatchNorm)\n",
      "res4h_branch2b         (Conv2D)\n",
      "bn4h_branch2b          (BatchNorm)\n",
      "res4h_branch2c         (Conv2D)\n",
      "bn4h_branch2c          (BatchNorm)\n",
      "res4i_branch2a         (Conv2D)\n",
      "bn4i_branch2a          (BatchNorm)\n",
      "res4i_branch2b         (Conv2D)\n",
      "bn4i_branch2b          (BatchNorm)\n",
      "res4i_branch2c         (Conv2D)\n",
      "bn4i_branch2c          (BatchNorm)\n",
      "res4j_branch2a         (Conv2D)\n",
      "bn4j_branch2a          (BatchNorm)\n",
      "res4j_branch2b         (Conv2D)\n",
      "bn4j_branch2b          (BatchNorm)\n",
      "res4j_branch2c         (Conv2D)\n",
      "bn4j_branch2c          (BatchNorm)\n",
      "res4k_branch2a         (Conv2D)\n",
      "bn4k_branch2a          (BatchNorm)\n",
      "res4k_branch2b         (Conv2D)\n",
      "bn4k_branch2b          (BatchNorm)\n",
      "res4k_branch2c         (Conv2D)\n",
      "bn4k_branch2c          (BatchNorm)\n",
      "res4l_branch2a         (Conv2D)\n",
      "bn4l_branch2a          (BatchNorm)\n",
      "res4l_branch2b         (Conv2D)\n",
      "bn4l_branch2b          (BatchNorm)\n",
      "res4l_branch2c         (Conv2D)\n",
      "bn4l_branch2c          (BatchNorm)\n",
      "res4m_branch2a         (Conv2D)\n",
      "bn4m_branch2a          (BatchNorm)\n",
      "res4m_branch2b         (Conv2D)\n",
      "bn4m_branch2b          (BatchNorm)\n",
      "res4m_branch2c         (Conv2D)\n",
      "bn4m_branch2c          (BatchNorm)\n",
      "res4n_branch2a         (Conv2D)\n",
      "bn4n_branch2a          (BatchNorm)\n",
      "res4n_branch2b         (Conv2D)\n",
      "bn4n_branch2b          (BatchNorm)\n",
      "res4n_branch2c         (Conv2D)\n",
      "bn4n_branch2c          (BatchNorm)\n",
      "res4o_branch2a         (Conv2D)\n",
      "bn4o_branch2a          (BatchNorm)\n",
      "res4o_branch2b         (Conv2D)\n",
      "bn4o_branch2b          (BatchNorm)\n",
      "res4o_branch2c         (Conv2D)\n",
      "bn4o_branch2c          (BatchNorm)\n",
      "res4p_branch2a         (Conv2D)\n",
      "bn4p_branch2a          (BatchNorm)\n",
      "res4p_branch2b         (Conv2D)\n",
      "bn4p_branch2b          (BatchNorm)\n",
      "res4p_branch2c         (Conv2D)\n",
      "bn4p_branch2c          (BatchNorm)\n",
      "res4q_branch2a         (Conv2D)\n",
      "bn4q_branch2a          (BatchNorm)\n",
      "res4q_branch2b         (Conv2D)\n",
      "bn4q_branch2b          (BatchNorm)\n",
      "res4q_branch2c         (Conv2D)\n",
      "bn4q_branch2c          (BatchNorm)\n",
      "res4r_branch2a         (Conv2D)\n",
      "bn4r_branch2a          (BatchNorm)\n",
      "res4r_branch2b         (Conv2D)\n",
      "bn4r_branch2b          (BatchNorm)\n",
      "res4r_branch2c         (Conv2D)\n",
      "bn4r_branch2c          (BatchNorm)\n",
      "res4s_branch2a         (Conv2D)\n",
      "bn4s_branch2a          (BatchNorm)\n",
      "res4s_branch2b         (Conv2D)\n",
      "bn4s_branch2b          (BatchNorm)\n",
      "res4s_branch2c         (Conv2D)\n",
      "bn4s_branch2c          (BatchNorm)\n",
      "res4t_branch2a         (Conv2D)\n",
      "bn4t_branch2a          (BatchNorm)\n",
      "res4t_branch2b         (Conv2D)\n",
      "bn4t_branch2b          (BatchNorm)\n",
      "res4t_branch2c         (Conv2D)\n",
      "bn4t_branch2c          (BatchNorm)\n",
      "res4u_branch2a         (Conv2D)\n",
      "bn4u_branch2a          (BatchNorm)\n",
      "res4u_branch2b         (Conv2D)\n",
      "bn4u_branch2b          (BatchNorm)\n",
      "res4u_branch2c         (Conv2D)\n",
      "bn4u_branch2c          (BatchNorm)\n",
      "res4v_branch2a         (Conv2D)\n",
      "bn4v_branch2a          (BatchNorm)\n",
      "res4v_branch2b         (Conv2D)\n",
      "bn4v_branch2b          (BatchNorm)\n",
      "res4v_branch2c         (Conv2D)\n",
      "bn4v_branch2c          (BatchNorm)\n",
      "res4w_branch2a         (Conv2D)\n",
      "bn4w_branch2a          (BatchNorm)\n",
      "res4w_branch2b         (Conv2D)\n",
      "bn4w_branch2b          (BatchNorm)\n",
      "res4w_branch2c         (Conv2D)\n",
      "bn4w_branch2c          (BatchNorm)\n",
      "res5a_branch2a         (Conv2D)\n",
      "bn5a_branch2a          (BatchNorm)\n",
      "res5a_branch2b         (Conv2D)\n",
      "bn5a_branch2b          (BatchNorm)\n",
      "res5a_branch2c         (Conv2D)\n",
      "res5a_branch1          (Conv2D)\n",
      "bn5a_branch2c          (BatchNorm)\n",
      "bn5a_branch1           (BatchNorm)\n",
      "res5b_branch2a         (Conv2D)\n",
      "bn5b_branch2a          (BatchNorm)\n",
      "res5b_branch2b         (Conv2D)\n",
      "bn5b_branch2b          (BatchNorm)\n",
      "res5b_branch2c         (Conv2D)\n",
      "bn5b_branch2c          (BatchNorm)\n",
      "res5c_branch2a         (Conv2D)\n",
      "bn5c_branch2a          (BatchNorm)\n",
      "res5c_branch2b         (Conv2D)\n",
      "bn5c_branch2b          (BatchNorm)\n",
      "res5c_branch2c         (Conv2D)\n",
      "bn5c_branch2c          (BatchNorm)\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keven/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:95: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/keven/miniconda3/envs/tensorflow/lib/python3.6/site-packages/keras/engine/training.py:2033: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    }
   ],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=2, \n",
    "            layers=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save weights\n",
    "# Typically not needed because callbacks save after every epoch\n",
    "# Uncomment to save manually\n",
    "# model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes.h5\")\n",
    "# model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from  /home/keven/data_science_bowl_2018/Mask_RCNN/logs/shapes20180216T1319/mask_rcnn_shapes_0002.h5\n"
     ]
    }
   ],
   "source": [
    "class InferenceConfig(ShapesConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = model.find_last()[1]\n",
    "\n",
    "# Load trained weights (fill in path to trained weights here)\n",
    "assert model_path != \"\", \"Provide path to trained weights\"\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_image           shape: (128, 128, 3)         min:    2.00000  max:  181.00000\n",
      "image_meta               shape: (12,)                 min:    0.00000  max:  128.00000\n",
      "gt_class_id              shape: (1,)                  min:    2.00000  max:    2.00000\n",
      "gt_bbox                  shape: (1, 4)                min:   20.00000  max:  110.00000\n",
      "gt_mask                  shape: (128, 128, 1)         min:    0.00000  max:    1.00000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd4AAAHVCAYAAABfWZoAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nADD0/wFcgYrOEjN2kgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f311e4fedd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 images\n",
      "image                    shape: (128, 128, 3)         min:    2.00000  max:  181.00000\n",
      "molded_images            shape: (1, 128, 128, 3)      min: -101.90000  max:   57.30000\n",
      "image_metas              shape: (1, 12)               min:    0.00000  max:  128.00000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd4AAAHVCAYAAABfWZoAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f31497a6320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = model.detect([original_image], verbose=1)\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'], ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP:  0.5430555572112401\n"
     ]
    }
   ],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    APs.append(AP)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
